Below is a plain-style, no-shortcuts Python script that:
	1.	Parses only the last line in INSMonitor_prs.service.txt
	2.	Builds an INS dictionary whose keys match the field names you listed (with sub-dictionaries for vectors)
	3.	Checks every numeric component against user-changeable limits
	4.	Calculates mean + sample-σ for each numeric item that appeared any time in the last 5 minutes of the log
	5.	Writes two tidy JSON files so you can inspect or load the results elsewhere.

Save it, adjust the paths and the RANGES dictionary, then run it with plain Python 3.  All loops are written out in full—no list- or dict-comprehensions, no one-liners.

#!/usr/bin/env python3
"""
parse_ins_log.py
– Build a structured dict from the very last log entry
– Range-check each numeric component
– Compute mean & sdev for everything that appeared in the final 5-minute window
– Spit the results into two JSON files
"""

import re
import json
import math
import time
from pathlib import Path
from datetime import datetime, timezone

# ---------------------------------------------------------------------------
# ------------- USER-TWEAKABLE SETTINGS ------------- #
LOG_FILE  = Path("/mnt/data/INSMonitor_prs.service.txt")     # change if needed
LAST_OUT  = Path("/mnt/data/INS_last_entry.json")
STATS_OUT = Path("/mnt/data/INS_stats_5min.json")
WINDOW_SEC = 5 * 60                                          # “last 5 minutes”

# Acceptable limits for every numeric component you care about.
# Insert or change entries at will.
RANGES = {
    "LLA":             {"Lat": (-90, 90),    "Lon": (-180, 180), "Alt": (-100, 10000)},
    "RPY_EKF":         {"R":  (-180, 180),   "P":  (-90,  90),   "Y":  (   0, 360)},
    "XYZ_Accel_IMU":   {"X": (-10,   10),    "Y": (-10,   10),   "Z": (-10,   10)},
    "XYZ_AngVel_IMU":  {"X": (-20,   20),    "Y": (-20,   20),   "Z": (-20,   20)},
    "XYZ_AngVel_Bias": {"X": (-10,   10),    "Y": (-10,   10),   "Z": (-10,   10)}
}
# ---------------------------------------------------------------------------


def _epoch_to_iso(epoch_sec: float) -> str:
    return datetime.fromtimestamp(epoch_sec, tz=timezone.utc).isoformat()


def _safe_float(text: str):
    try:
        return float(text)
    except ValueError:
        return text


def parse_line(line: str):
    """
    Extract epoch-seconds plus every field we care about.
    Returns (epoch, data-dict) or None if the line doesn't match.
    """
    # -- epoch seconds appear just before the second colon
    m_epoch = re.search(r':\s*(\d+\.\d+):', line)
    if m_epoch is None:
        return None
    epoch = float(m_epoch.group(1))

    data = {}

    # Regular-expressions keyed by our canonical field names
    patterns = {
        "LLA":               r"LLA:\s*\{([^}]*)\}",
        "HDG":               r"HDG:\s*\{([^}]*)\}",
        "HDG_Lock":          r"HDG Lock:\s*\{([^}]*)\}",
        "RPY_EKF":           r"RPY \(EKF\):\s*\{([^}]*)\}",
        "XYZ_AngVel_Bias":   r"XYZ AngVel Bias \(EKF\):\s*\{([^}]*)\}",
        "ERR":               r"ERR:\s*([0-9.+-]+)",
        "INS_Status":        r"INS Status:\s*\{([^}]*)\}",
        "GPS_Status":        r"GPS Status:\s*\{([^}]*)\}",
        "VEL":               r"VEL:\s*\{([^}]*)\}",
        "COG":               r"COG:\s*\{([^}]*)\}",
        "XYZ_Accel_IMU":     r"XYZ Accel\(IMU\):\s*\{([^}]*)\}",
        "XYZ_AngVel_IMU":    r"XYZ AngVel \(IMU\):\s*\{([^}]*)\}",
    }

    # Extract every pattern one by one (no one-liners)
    for key in patterns:
        pat = patterns[key]
        match = re.search(pat, line)
        if match:
            raw = match.group(1).strip()

            # Vector or scalar?
            if "," in raw:
                pieces = raw.split(",")
                values = []
                for piece in pieces:
                    piece = piece.strip()
                    if piece.lower() == "true":
                        values.append(True)
                    elif piece.lower() == "false":
                        values.append(False)
                    else:
                        values.append(_safe_float(piece))
                data[key] = values
            else:
                if raw.lower() == "true":
                    data[key] = True
                elif raw.lower() == "false":
                    data[key] = False
                else:
                    data[key] = _safe_float(raw)

    return epoch, data


def build_ins_dict(raw: dict):
    """Re-shape raw vectors into labelled sub-dictionaries."""
    ins = {}

    if "LLA" in raw and len(raw["LLA"]) == 3:
        ins["LLA"] = {"Lat": raw["LLA"][0],
                      "Lon": raw["LLA"][1],
                      "Alt": raw["LLA"][2]}

    if "HDG" in raw:
        ins["HDG"] = raw["HDG"]

    if "HDG_Lock" in raw:
        ins["HDG_Lock"] = bool(raw["HDG_Lock"])

    if "RPY_EKF" in raw and len(raw["RPY_EKF"]) == 3:
        ins["RPY_EKF"] = {"R": raw["RPY_EKF"][0],
                          "P": raw["RPY_EKF"][1],
                          "Y": raw["RPY_EKF"][2]}

    if "XYZ_AngVel_Bias" in raw and len(raw["XYZ_AngVel_Bias"]) == 3:
        ins["XYZ_AngVel_Bias"] = {"X": raw["XYZ_AngVel_Bias"][0],
                                  "Y": raw["XYZ_AngVel_Bias"][1],
                                  "Z": raw["XYZ_AngVel_Bias"][2]}

    if "ERR" in raw:
        ins["ERR"] = raw["ERR"]

    if "INS_Status" in raw:
        ins["INS_Status"] = raw["INS_Status"]

    if "GPS_Status" in raw:
        ins["GPS_Status"] = raw["GPS_Status"]

    if "VEL" in raw:
        ins["VEL"] = raw["VEL"]

    if "COG" in raw:
        ins["COG"] = raw["COG"]

    if "XYZ_Accel_IMU" in raw and len(raw["XYZ_Accel_IMU"]) == 3:
        ins["XYZ_Accel_IMU"] = {"X": raw["XYZ_Accel_IMU"][0],
                                "Y": raw["XYZ_Accel_IMU"][1],
                                "Z": raw["XYZ_Accel_IMU"][2]}

    if "XYZ_AngVel_IMU" in raw and len(raw["XYZ_AngVel_IMU"]) == 3:
        ins["XYZ_AngVel_IMU"] = {"X": raw["XYZ_AngVel_IMU"][0],
                                 "Y": raw["XYZ_AngVel_IMU"][1],
                                 "Z": raw["XYZ_AngVel_IMU"][2]}
    return ins


def check_ranges(ins: dict, ranges: dict):
    """
    Return a mirror structure with True/False flags
    stating whether each component lies inside its limits.
    """
    results = {}
    for field in ranges:
        if field in ins:
            sub_result = {}
            for comp in ranges[field]:
                if comp in ins[field]:
                    lo, hi = ranges[field][comp]
                    value = ins[field][comp]
                    sub_result[comp] = (lo <= value <= hi)
            if sub_result:
                results[field] = sub_result
    return results


def calc_stats(window_records, last_epoch):
    """
    Compute mean + sample std-dev for each numeric scalar /
    each component of every numeric vector.
    """
    # buckets = {field -> list-of-values OR list-of-vectors}
    buckets = {}
    for epoch, data in window_records:
        if epoch <= last_epoch and epoch >= last_epoch - WINDOW_SEC:
            for field in data:
                value = data[field]

                # collect only numeric items
                if isinstance(value, list):
                    numeric = True
                    for v in value:
                        numeric = numeric and isinstance(v, (int, float))
                    if not numeric:
                        continue
                elif not isinstance(value, (int, float)):
                    continue

                if field not in buckets:
                    buckets[field] = []
                buckets[field].append(value)

    # stats = {field -> {comp -> {"mean": m, "std": s}}}
    stats = {}
    for field in buckets:
        values = buckets[field]
        # vector or scalar?
        if isinstance(values[0], list):
            length = len(values[0])

            # choose component labels
            if field == "LLA":
                labels = ["Lat", "Lon", "Alt"]
            elif field.startswith("RPY"):
                labels = ["R", "P", "Y"]
            else:
                labels = ["X", "Y", "Z"]

            comp_stats = {}
            for idx in range(length):
                comp_vals = []
                for vec in values:
                    comp_vals.append(vec[idx])

                mean_val = sum(comp_vals) / len(comp_vals)

                if len(comp_vals) > 1:
                    variance = 0.0
                    for num in comp_vals:
                        variance += (num - mean_val) ** 2
                    variance /= (len(comp_vals) - 1)
                    std_val = math.sqrt(variance)
                else:
                    std_val = 0.0

                comp_stats[labels[idx]] = {"mean": mean_val, "std": std_val}

            stats[field] = comp_stats
        else:
            # scalar
            mean_val = sum(values) / len(values)
            if len(values) > 1:
                variance = 0.0
                for num in values:
                    variance += (num - mean_val) ** 2
                variance /= (len(values) - 1)
                std_val = math.sqrt(variance)
            else:
                std_val = 0.0
            stats[field] = {"mean": mean_val, "std": std_val}

    return stats


# ---------------------------------------------------------------------------
#                           MAIN WORKFLOW
# ---------------------------------------------------------------------------

records = []

with LOG_FILE.open(errors="ignore") as fh:
    for line in fh:
        parsed = parse_line(line)
        if parsed is not None:
            records.append(parsed)

if not records:
    raise RuntimeError("No parseable lines found in the log!")

# Sort just in case
records.sort(key=lambda tup: tup[0])

# Last entry & its dictionary
last_epoch, raw_last = records[-1]
ins_dict = build_ins_dict(raw_last)

# Range checks
range_results = check_ranges(ins_dict, RANGES)

# Stats over the trailing WINDOW_SEC seconds
stats_dict = calc_stats(records, last_epoch)

# ---------------------------------------------------------------------------
#                      WRITE EVERYTHING TO JSON
# ---------------------------------------------------------------------------
last_output = {
    "timestamp_epoch": last_epoch,
    "timestamp_iso":   _epoch_to_iso(last_epoch),
    "INS":             ins_dict,
    "range_checks":    range_results
}
LAST_OUT.write_text(json.dumps(last_output, indent=2))

stats_output = {
    "window_start_epoch": last_epoch - WINDOW_SEC,
    "window_end_epoch":   last_epoch,
    "window_start_iso":   _epoch_to_iso(last_epoch - WINDOW_SEC),
    "window_end_iso":     _epoch_to_iso(last_epoch),
    "stats":              stats_dict
}
STATS_OUT.write_text(json.dumps(stats_output, indent=2))

print("✓  Saved:")
print("   • Latest entry  →", LAST_OUT)
print("   • 5-minute stats →", STATS_OUT)

What you get
	•	INS_last_entry.json – the single, fully-expanded dictionary for the final log line, plus pass/fail flags for every limit you set in RANGES.
	•	INS_stats_5min.json – mean and sample‐σ for each numeric field/component that occurred during the last 5 minutes of log time (as defined by the epoch value embedded in each line).

Both files live right beside the script (or wherever you point LAST_OUT / STATS_OUT). Open them in any text editor, feed them to a downstream program, or pretty-print them with jq.

Try it now:

python3 parse_ins_log.py
cat /mnt/data/INS_last_entry.json | jq .



Tweak RANGES or WINDOW_SEC any time—no other lines need to change. If you’d like this to tail the log live or to export CSV/Excel, just let me know and we’ll extend it.